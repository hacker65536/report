# kubernetes meet up tokyo 18

kubebuilder/controller-runtime 入門
-- 



Container-native ingress controller with kubebuilder/(Admission Webhook) 
--

GKE igress(nomal)

vpc ネイティブの有効化　
- GKEクラスタ構築時に設定
- 今後はデフォルトで有効化されていく予定
- container-native load balancing 利用時は必須

pod宛に転送


instance groups(IGs) vs network endopoint groups (NEGs)

Nginx ingress

L7を処理するpodを作成してオフロードする

必要なりソース
- deployment (nginx ingress)
- Service (type: LoadBalancer)
- (HorizontalPodAutoscaler)


GKEとの違い
1. Deploymentやserviceなどをいちいちつくる必要がある
2. ingress classを利用しないとリソースごとに分離できない


GKEの場合ingressリソースごとにGCLBが作成される
Nignx ingressの場合は必要ならばingress-classで分割


AKE Ingress controller v1 (2017-12-

ingress controller = l7 lb を作る

いい感じにリソースを操作することでGKELIKEなIngressを実現

ControllerによりオペレーションをProgrammingで自動化


AKE Ingress v2(container-native)

L7LBを外側に構築

v1 L4LB

v2 L4LB + VM (nginx + calico)

calicoによりVMからPodへの疎通性確保

Calico大変そう？
データセンターなどでバッティングしそう
- vmに対して伝播しているだけ
- nginx ingressからpodへの疎通性確保

バグ多そう　実装とても大変そう

既存のNginx-ingressを利用することで大幅な実装コストダウン
- Nginx-ingressが各エントリはもともとPodのIPAddress
- あくまで構築・運用の範囲をController化しただけ

Heat*のAPIを叩いてVMClusterが起動するのを待つ
- レプリカ数など管理するものが多い
  - IngressリソースのAnnotationに保持？
- コントロール対象（関心事）は狭めたい

HeatStackとDeploymentの違い

HeatStackController
VMClusterを作ることだけに集中
ステータスにHeatのOutputと動機
StatusSubresourceを有効化
レプリカ以外が書き換えられると困る・・・。
すべてのVMを再構築してしまう可能性（Heatの仕様）
ValidatingWebhook


admissionrequest

メルカリのマイクロサービスにおける Kubernetes manifest の運用
--

service 100+

contributer 200+

rbac for service A team

### kubernetes yaml

write -> file -> apply ->k8s

1. pr
2. gitops w/ kubectl
3. monorepo
4. directories


kubernetes object management

imperative commands ( kubectl run nginx --image nginx
imperative object configuration ( kubectl create -f nginx.yaml
declarative object configuration ( kubectl apply -f derectory/

pull requests
- easy to review
- easy to track operations and changes
- easy to recover from unexpected things 
  - just reverting
  

gitops(weaveworks

start 6servers 

there are 6servers

k8s = live objects
source of truth = github

diff & sync

our ci pipline which runs "kubectl applh" based on the changes is triggered by merging pull requests
however, in some resources(job etc), we want to apply

monorepo vs polyrepo

monorepos : please dont't
monorepos: please do!

advantages
- easy to share yaml code
- easy to be reviewed by central team
- easy to be managed by central team
- easy to set up ci pipeline

disadvantages
- take account into repo scale up
- take account into delegation of authority
- ci/cd No independence in each team


helm installorder 

kustomize

poddisruptionbudget(PDB)

github codeowners feature


stein
HCLのconfigration lint


みんなのdocker kubernetes

LT 1: controller-runtime で拡張可能な Controller を作った話
--

whitebox controller

extensible reconciler


implementing controller in bash(1)

LT 2: とある30秒でできるKubernetes + Istio 開発環境	
--
snapcraft.io


snap
$ sudo snap install microk8s --classic

addons
$ microk8s.enable istio



snapcraft.io

LT 3: Rekcurdによる機械学習モジュールの運用について
--

MLOps

RekcurdFeatures


LT 4: Service Mesh Dayの報告(仮)
--

why istio and envoy are the future of networking for distributed systems
- app mesh
- getenvoy
  - getenvoy.io
  
envoy as the standard data plane and where its going

cloud next

- traffic director
- control plane for service mesh ( vs aws appmesh)

envoy-based internal LB
managed envoy as an internal load balancer

cloud service mesh

service mesh day at kubecon na 2019

LT 5: Submarinerで考えるマルチクラウド	
--

マルチクラウドにおけるKuernetes
Docker enterpriseEdition
Anthos


Submarinerとは

Rancherlabsがオープンソースで公開したK8sクラスタ間のネットワーク

さわった結果
Githubー＞Helmであっという間

AWS（EKS）、Azure（AKS)
GKEは一部正常に動かない

今の所CNI構成に依存する

AzureCNI　PodのIPはVM（WorkerNode)と同じセグメントから

